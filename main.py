import sqlite3
if sqlite3.sqlite_version_info < (3, 35, 0):
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader, TextLoader
from streaming import StreamHandler
import utils
from collections import deque 

# ê° í™•ìž¥ìž ë³„ë¡œ ë¬¸ì„œ ë¡œë” ì •ì˜
loaders = {
    'pdf': {'loader':PyMuPDFLoader, 'kwargs': {}},
    'txt': {'loader':TextLoader, 'kwargs': {'autodetect_encoding': True}}
}

st.title("ë‹¹ë‡¨ í™˜ìžë“¤ì„ ìœ„í•œ ì±—ë´‡ ðŸ’Š")
st.sidebar.markdown('[![](https://img.shields.io/badge/7ì¡°_ì†ŒìŠ¤ì½”ë“œ_ë³´ëŸ¬ê°€ê¸°-red?logo=github)](https://github.com/int11/langchaine_medicine/blob/main/main.py)')

# openai key input gui. ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ë©ˆì¶¤ ìžˆìœ¼ë©´ ê³„ì† ì§„í–‰
model_name = utils.configure_openai()

# qa_chain ëª¨ë¸ ì •ì˜.
# ìµœì´ˆë¡œ í•œë²ˆ ì •ì˜í•˜ê³  application scope ë³€ìˆ˜(stê°ì²´)ë¡œ ì €ìž¥í•´ë‘ .  
# OpenAIEmbeddingsëŠ” OpenAIì˜ GPT-3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
if not hasattr(st, "qa_chain"):
    with st.spinner('ë‹µë³€ì— í•„ìš”í•œ ë¬¸ì„œë¥¼ ì½ê³  ìžˆìŠµë‹ˆë‹¤. ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'):
    
        documents = []
        for file_type, value in loaders.items():
            loader = value['loader']
            loader_kwargs = value['kwargs']

            loader = DirectoryLoader(path=f"data/{file_type}", glob=f"**/*.{file_type}",loader_cls=loader, loader_kwargs=loader_kwargs)
            documents.extend(loader.load())

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ê¸° ì •ì˜
        embedding = OpenAIEmbeddings()
        
        st.vectordb = Chroma(embedding_function=embedding) if len(documents) == 0 else Chroma.from_documents(documents=documents, embedding=embedding)

        retriever = st.vectordb.as_retriever()

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        llm = ChatOpenAI(model_name=model_name, temperature=0, streaming=True)
        
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=True
        )


# User file uploader
uploaded_files = st.sidebar.file_uploader(label='íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”', type=loaders.keys(), accept_multiple_files=True)
    
for uploaded_file in uploaded_files:
    _, extension = os.path.splitext(uploaded_file.name)
    extension = extension[1:] # remove the dot
     
    file_path = f"data/{extension}/{uploaded_file.name}"

    if not os.path.exists(file_path):
        with st.spinner('ì‚¬ìš©ìž ë¬¸ì„œë¥¼ ì½ê³  ìžˆìŠµë‹ˆë‹¤. ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'):
            # Save the file
            with open(file_path, 'wb') as f:
                f.write(uploaded_file.getvalue())

            loader = loaders[extension]['loader']
            loader_kwargs = loaders[extension]['kwargs']
            document = loader(file_path, **loader_kwargs).load()
            st.vectordb.add_documents(document)

            
#chat gui
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ë‡¨ë³‘ì— ê´€í•œ ì§ˆë¬¸ì„ ì£¼ì„¸ìš”!"}]

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

user_query = st.chat_input(placeholder="ë‹¹ë‡¨ ê´€ë ¨ ì§ˆë¬¸í•˜ì„¸ìš”!")

if user_query:
    with st.chat_message("user"):
        st.session_state.messages.append({"role": "user", "content": user_query})
        st.write(user_query)

    with st.chat_message("assistant"):
        st_cb = StreamHandler(st.empty())
        result = st.qa_chain.invoke(
            {"query":user_query},
            {"callbacks": [st_cb]}
        )
        response = result["result"]
        st.session_state.messages.append({"role": "assistant", "content": response})

        # to show references
        for idx, doc in enumerate(result['source_documents'],1):
            filename = os.path.basename(doc.metadata['source'])
            _, ext = os.path.splitext(filename)
        
            if ext == '.txt':
                ref_title = f":blue[Reference {idx}: *{filename}*]"
            elif ext == '.pdf':
                page_num = doc.metadata['page']
                ref_title = f":blue[Reference {idx}: *{filename} - page.{page_num}*]"

            with st.popover(ref_title):
                st.caption(doc.page_content)